{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef36f535-4bdc-4e2b-a22a-179372324b26",
   "metadata": {},
   "source": [
    "![walmartecomm](walmartecomm.jpg)\n",
    "\n",
    "Walmart is the biggest retail store in the United States. Just like others, they have been expanding their e-commerce part of the business. By the end of 2022, e-commerce represented a roaring $80 billion in sales, which is 13% of total sales of Walmart. One of the main factors that affects their sales is public holidays, like the Super Bowl, Labour Day, Thanksgiving, and Christmas. \n",
    "\n",
    "In this project, you have been tasked with creating a data pipeline for the analysis of supply and demand around the holidays, along with conducting a preliminary analysis of the data. You will be working with two data sources: grocery sales and complementary data. You have been provided with the `grocery_sales` table in `PostgreSQL` database with the following features:\n",
    "\n",
    "# `grocery_sales`\n",
    "- `\"index\"` - unique ID of the row\n",
    "- `\"Store_ID\"` - the store number\n",
    "- `\"Date\"` - the week of sales\n",
    "- `\"Weekly_Sales\"` - sales for the given store\n",
    "\n",
    "Also, you have the `extra_data.parquet` file that contains complementary data:\n",
    "\n",
    "# `extra_data.parquet`\n",
    "- `\"IsHoliday\"` - Whether the week contains a public holiday - 1 if yes, 0 if no.\n",
    "- `\"Temperature\"` - Temperature on the day of sale\n",
    "- `\"Fuel_Price\"` - Cost of fuel in the region\n",
    "- `\"CPI\"` â€“ Prevailing consumer price index\n",
    "- `\"Unemployment\"` - The prevailing unemployment rate\n",
    "- `\"MarkDown1\"`, `\"MarkDown2\"`, `\"MarkDown3\"`, `\"MarkDown4\"` - number of promotional markdowns\n",
    "- `\"Dept\"` - Department Number in each store\n",
    "- `\"Size\"` - size of the store\n",
    "- `\"Type\"` - type of the store (depends on `Size` column)\n",
    "\n",
    "You will need to merge those files and perform some data manipulations. The transformed DataFrame can then be stored as the `clean_data` variable containing the following columns:\n",
    "- `\"Store_ID\"`\n",
    "- `\"Month\"`\n",
    "- `\"Dept\"`\n",
    "- `\"IsHoliday\"`\n",
    "- `\"Weekly_Sales\"`\n",
    "- `\"CPI\"`\n",
    "- \"`\"Unemployment\"`\"\n",
    "\n",
    "After merging and cleaning the data, you will have to analyze monthly sales of Walmart and store the results of your analysis as the `agg_data` variable that should look like:\n",
    "\n",
    "|  Month | Weekly_Sales  | \n",
    "|---|---|\n",
    "| 1.0  |  33174.178494 |\n",
    "|  2.0 |  34333.326579 |\n",
    "|  ... | ...  |  \n",
    "\n",
    "Finally, you should save the `clean_data` and `agg_data` as the csv files.\n",
    "\n",
    "It is recommended to use `pandas` for this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d6e4d2",
   "metadata": {},
   "source": [
    "Project:\n",
    "\n",
    "Build a data pipeline using custom functions to extract, transform, aggregate, and load e-commerce data. The SQL query for grocery_sales and the extract() function have already been implemented for you.\n",
    "\n",
    "To start the project, run the first two cells, then proceed with the following steps:\n",
    "\n",
    "1. Implement a function named transform() with one argument, taking merged_df as input, filling missing numerical values (using any method of your choice), adding a column \"Month\", keeping the rows where the weekly sales are over $10,000 and drops the unnecessary columns. Ultimately, it should return a DataFrame and be stored as the clean_data variable.\n",
    "\n",
    "2. Implement the function avg_weekly_sales_per_month with one argument (the cleaned data). This function will calculate the average monthly sales. For implementing this function you must select the \"Month\" and \"Weekly_Sales\" columns as they are the only ones needed for this analysis, then create a chain operation with groupby(), agg(), reset_index(), and round() functions, then group by the \"Month\" column and calculate the average monthly sales, then call reset_index() to start a new index order and finally round the results to two decimal places.\n",
    "\n",
    "3. Create a function called load() that takes the cleaned and aggregated DataFrames, and their paths, and saves them as clean_data.csv and agg_data.csv respectively, without an index.\n",
    "\n",
    "4. Lastly, define a validation() function that checks whether the two csv files from the load() exist in the current working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d64ff1-a4ca-4a82-a8b4-e210244dedc1",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 82,
    "lastExecutedAt": 1753818872277,
    "lastExecutedByKernel": "a93ff7d2-8fa2-49af-85dd-7bd4961dbd29",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pandas as pd\nimport os\n\n# Extract function is already implemented for you \ndef extract(store_data, extra_data):\n    extra_df = pd.read_parquet(extra_data)\n    merged_df = store_data.merge(extra_df, on = \"index\")\n    return merged_df\n\n# Call the extract() function and store it as the \"merged_df\" variable\nmerged_df = extract(grocery_sales, \"extra_data.parquet\")"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Extract function\n",
    "def extract():\n",
    "    merged_df = pd.read_csv(\"merged_df.csv\")\n",
    "    return merged_df\n",
    "\n",
    "# Call the extract() function and store it as the \"merged_df\" variable\n",
    "merged_df = extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8e4b7c84-8ad2-4b4a-ace9-f2de35bc5108",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 56,
    "lastExecutedAt": 1753818872334,
    "lastExecutedByKernel": "a93ff7d2-8fa2-49af-85dd-7bd4961dbd29",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(merged_df.head())\nprint(merged_df.info())",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  Store_ID       Date  Dept  ...         CPI  Unemployment  Type      Size\n",
      "0      0         1 2010-02-05     1  ...  211.096358         8.106   3.0  151315.0\n",
      "1      1         1 2010-02-05    26  ...  211.096358         8.106   3.0  151315.0\n",
      "2      2         1 2010-02-05    17  ...  211.096358         8.106   3.0  151315.0\n",
      "3      3         1 2010-02-05    45  ...  211.096358           NaN   3.0  151315.0\n",
      "4      4         1 2010-02-05    28  ...  211.096358           NaN   3.0  151315.0\n",
      "\n",
      "[5 rows x 17 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 231522 entries, 0 to 231521\n",
      "Data columns (total 17 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   index         231522 non-null  int64         \n",
      " 1   Store_ID      231522 non-null  int64         \n",
      " 2   Date          231483 non-null  datetime64[ns]\n",
      " 3   Dept          231522 non-null  int64         \n",
      " 4   Weekly_Sales  231484 non-null  float64       \n",
      " 5   IsHoliday     231522 non-null  int64         \n",
      " 6   Temperature   231522 non-null  float64       \n",
      " 7   Fuel_Price    231522 non-null  float64       \n",
      " 8   MarkDown1     231522 non-null  float64       \n",
      " 9   MarkDown2     231522 non-null  float64       \n",
      " 10  MarkDown3     231522 non-null  float64       \n",
      " 11  MarkDown4     231521 non-null  float64       \n",
      " 12  MarkDown5     231521 non-null  float64       \n",
      " 13  CPI           231475 non-null  float64       \n",
      " 14  Unemployment  231485 non-null  float64       \n",
      " 15  Type          231521 non-null  float64       \n",
      " 16  Size          231521 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(12), int64(4)\n",
      "memory usage: 31.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Overview of the merged_df content\n",
    "print(merged_df.head())\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6d3c25e2-e7d8-4c33-9be0-d45f03b2cf43",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1753818872542,
    "lastExecutedByKernel": "a93ff7d2-8fa2-49af-85dd-7bd4961dbd29",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the transform() function with one parameter: \"raw_data\"\ndef transform(raw_data):\n    \n    # Drops rows with Weekly_Sales less equal than 10000 and unuseful columns\n    raw_data = raw_data.dropna(subset='Date').loc[raw_data['Weekly_Sales'] > 10000, :]\n    \n    na_values_dict = {  # Dict with values to replace into the df\n        'Date': raw_data['Date'].max() + pd.DateOffset(days=1),    # One day next to the max day into the df\n        'Weekly_Sales': raw_data['Weekly_Sales'].mean(),\n        'CPI': raw_data['CPI'].mean(),\n        'Unemployment': raw_data['Unemployment'].mean(),\n    }\n    \n    # Fill NA fields with the values specified above\n    raw_data = raw_data.fillna(value=na_values_dict)\n    \n    # Creation of the new Month column\n    raw_data['Month'] = raw_data['Date'].dt.month\n    \n    return raw_data.loc[:, ['Weekly_Sales', 'Month']]"
   },
   "outputs": [],
   "source": [
    "# Create the transform() \n",
    "def transform(raw_data):\n",
    "    \n",
    "    # Drops rows with Weekly_Sales less equal than 10000\n",
    "    raw_data = raw_data.dropna(subset='Date').loc[raw_data['Weekly_Sales'] > 10000, :]\n",
    "    \n",
    "    na_values_dict = {  # Dict with values to replace into the df\n",
    "        'Weekly_Sales': raw_data['Weekly_Sales'].mean(),\n",
    "        'CPI': raw_data['CPI'].mean(),\n",
    "        'Unemployment': raw_data['Unemployment'].mean(),\n",
    "    }\n",
    "    \n",
    "    # Fill NA fields with the values specified above\n",
    "    raw_data = raw_data.fillna(value=na_values_dict)\n",
    "    \n",
    "    # Creation of the new Month column\n",
    "    raw_data['Month'] = raw_data['Date'].dt.month\n",
    "    \n",
    "    return raw_data.loc[:, ['Weekly_Sales', 'Month', 'CPI', 'Unemployment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "620b7289-06cd-4205-be9e-a50dc8d36cf0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1753818872594,
    "lastExecutedByKernel": "a93ff7d2-8fa2-49af-85dd-7bd4961dbd29",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the transform() function and pass the merged DataFrame\nclean_data = transform(merged_df)"
   },
   "outputs": [],
   "source": [
    "# Call the transform() function and pass the merged DataFrame\n",
    "clean_data = transform(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "86909a41-c9f1-4731-af7b-e594a677e9b4",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 60,
    "lastExecutedAt": 1753818872655,
    "lastExecutedByKernel": "a93ff7d2-8fa2-49af-85dd-7bd4961dbd29",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "clean_data.head()",
    "outputsMetadata": {
     "0": {
      "height": 249,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "534dd6da-16bb-4a9e-a43c-ff02ef799ea8",
        "nodeType": "const"
       },
       "quickFilterText": ""
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "Month": [
          2,
          2,
          2,
          2,
          2
         ],
         "Weekly_Sales": [
          24924.5,
          11737.12,
          13223.76,
          46729.77,
          21249.31
         ],
         "index": [
          0,
          1,
          2,
          5,
          6
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "integer"
          },
          {
           "name": "Weekly_Sales",
           "type": "number"
          },
          {
           "name": "Month",
           "type": "integer"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 5,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24924.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11737.12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13223.76</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>46729.77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21249.31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Weekly_Sales  Month\n",
       "0      24924.50      2\n",
       "1      11737.12      2\n",
       "2      13223.76      2\n",
       "5      46729.77      2\n",
       "6      21249.31      2"
      ]
     },
     "execution_count": 184,
     "metadata": {
      "application/com.datacamp.data-table.v2+json": {
       "status": "success"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b19b15e3-6624-47a9-927f-d3f12fe8212d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 55,
    "lastExecutedAt": 1753818872710,
    "lastExecutedByKernel": "a93ff7d2-8fa2-49af-85dd-7bd4961dbd29",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\ndef avg_weekly_sales_per_month(clean_data):\n    # Finding the average weekly sales per month using aggregate functions\n    result_data = clean_data.groupby('Month').agg(avg_weekly_sales_per_month=('Weekly_Sales', 'mean'))\n    \n    # Reindexing the rows and round the average weekly sales per month to two decimal places\n    return result_data.reset_index()['avg_weekly_sales_per_month'].round(2)"
   },
   "outputs": [],
   "source": [
    "# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\n",
    "def avg_weekly_sales_per_month(clean_data):\n",
    "    # Finding the average weekly sales per month using aggregate functions\n",
    "    result_data = clean_data.groupby('Month').agg(avg_weekly_sales_per_month=('Weekly_Sales', 'mean'))\n",
    "    \n",
    "    # Reindexing the rows and round the average weekly sales per month to two decimal places\n",
    "    return result_data.reset_index()['avg_weekly_sales_per_month'].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fe875e27-b0cf-4e52-994e-4ae1fe6e8876",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 55,
    "lastExecutedAt": 1753818872766,
    "lastExecutedByKernel": "a93ff7d2-8fa2-49af-85dd-7bd4961dbd29",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\nagg_data = avg_weekly_sales_per_month(clean_data)",
    "outputsMetadata": {
     "0": {
      "height": 50,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "534dd6da-16bb-4a9e-a43c-ff02ef799ea8",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\n",
    "agg_data = avg_weekly_sales_per_month(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "921cb123-3153-4334-bdeb-9bb227fdc530",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1753818872814,
    "lastExecutedByKernel": "a93ff7d2-8fa2-49af-85dd-7bd4961dbd29",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\ndef load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n    # Write your code here\n    full_data.to_csv(full_data_file_path)\n    agg_data.to_csv(agg_data_file_path)"
   },
   "outputs": [],
   "source": [
    "# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going \n",
    "# to be stored\n",
    "def load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n",
    "    full_data.to_csv(full_data_file_path)\n",
    "    agg_data.to_csv(agg_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f518ad5c-214e-474b-80bd-827b0c0e1536",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 249,
    "lastExecutedAt": 1753818873063,
    "lastExecutedByKernel": "a93ff7d2-8fa2-49af-85dd-7bd4961dbd29",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the load() function and pass the cleaned and aggregated DataFrames with their paths\nload(clean_data, 'clean_data.csv', agg_data, 'agg_data.csv')"
   },
   "outputs": [],
   "source": [
    "# Call the load() function and pass the cleaned and aggregated DataFrames with their paths\n",
    "load(clean_data, 'clean_data.csv', agg_data, 'agg_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "61b5f58a-70cb-40b3-bdbe-20b4079276e3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 50,
    "lastExecutedAt": 1753818873114,
    "lastExecutedByKernel": "a93ff7d2-8fa2-49af-85dd-7bd4961dbd29",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\nimport os\n\ndef validation(file_path):\n    # Write your code here\n    assert os.path.exists(file_path + 'clean_data.csv')\n    assert os.path.exists(file_path + 'agg_data.csv')"
   },
   "outputs": [],
   "source": [
    "# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\n",
    "import os\n",
    "\n",
    "def validation(file_path):\n",
    "    # Write your code here\n",
    "    assert os.path.exists(file_path + 'clean_data.csv') or os.path.exists(file_path + 'agg_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "df1659ff-41c4-4a92-9812-80c6eaa02b90",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1753818873167,
    "lastExecutedByKernel": "a93ff7d2-8fa2-49af-85dd-7bd4961dbd29",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\nvalidation('')"
   },
   "outputs": [],
   "source": [
    "# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\n",
    "validation('')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
